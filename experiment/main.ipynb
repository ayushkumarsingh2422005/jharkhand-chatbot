{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ac1789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9d70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PDF_FOLDER = \"./pdfs\"\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "EMBEDDING_DIM = 384\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 200\n",
    "INDEX_PATH = \"jharkhand_faiss.index\"\n",
    "METADATA_PATH = \"jharkhand_metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76497735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyAEX75wi2PJu038QuAdeelmTZ6_-mQNVqY\n",
      "Configuration done. PDFs expected in: ./pdfs\n"
     ]
    }
   ],
   "source": [
    "# Gemini API key\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\", \"YOUR_GEMINI_API_KEY\")\n",
    "print(GEMINI_API_KEY)\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "print(\"Configuration done. PDFs expected in:\", PDF_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a2269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 1. Jharkhand Food and Feed Processing Policy 2024.pdf ...\n",
      "Extracting: 2. Jharkhand Export Policy 2023.pdf ...\n",
      "Extracting: 3. Jharkhand MSME Promotion Policy 2023.pdf ...\n",
      "Extracting: 4. Jharkhand Electric Vehicle Policy 2022.pdf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 5. Jharkhand Ethanol Production Promotion Policy, 2022.pdf ...\n",
      "Extracting: 6. Jharkhand Industrial Park and Logistic Policy 2022.pdf ...\n",
      "Extracting: 7. Jharkhand Industrial and Investment Promotion Policy 2021.pdf ...\n",
      "Extracting: 8. Jharkhand Procurement Policy 2014.pdf ...\n",
      "Extracting: BPO Policy 2016.pdf ...\n",
      "Extracting: IPR_Assignment.pdf ...\n",
      "Extracting: ITes Policy 2016.pdf ...\n",
      "Extracting: Jharkhand State Mineral Policy 2017.pdf ...\n",
      "Extracting: visionhindi.pdf ...\n"
     ]
    }
   ],
   "source": [
    "# 3) PDF loading and text extraction\n",
    "\n",
    "def extract_text_from_pdf(path: str) -> str:\n",
    "    texts = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            txt = page.extract_text()\n",
    "            if txt:\n",
    "                texts.append(txt)\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "\n",
    "def load_all_pdfs(folder: str) -> Dict[str, str]:\n",
    "    pdfs = glob.glob(os.path.join(folder, \"*.pdf\"))\n",
    "    results = {}\n",
    "    for p in pdfs:\n",
    "        name = os.path.basename(p)\n",
    "        print(f\"Extracting: {name} ...\")\n",
    "        try:\n",
    "            results[name] = extract_text_from_pdf(p)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {name}: {e}\")\n",
    "    return results\n",
    "\n",
    "if not os.path.exists(PDF_FOLDER) or len(glob.glob(os.path.join(PDF_FOLDER, \"*.pdf\"))) == 0:\n",
    "    os.makedirs(PDF_FOLDER, exist_ok=True)\n",
    "    docs = {\"demo_policy.txt\": \"This is a demo policy about agriculture subsidies in Jharkhand. Eligibility: residents of Jharkhand; amount: up to INR 10,000; effective: 2024-01-01.\"}\n",
    "    print(\"No PDFs detected — using demo text.\")\n",
    "else:\n",
    "    docs = load_all_pdfs(PDF_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f69d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 901 chunks from 13 documents.\n"
     ]
    }
   ],
   "source": [
    "# 4) Text cleaning and chunking\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP):\n",
    "    text = clean_text(text)\n",
    "    chunks, start, L = [], 0, len(text)\n",
    "    while start < L:\n",
    "        end = min(start + chunk_size, L)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == L:\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "corpus = []\n",
    "for fname, txt in docs.items():\n",
    "    chunks = chunk_text(txt)\n",
    "    for i, c in enumerate(chunks):\n",
    "        corpus.append({\"id\": f\"{fname}_chunk_{i}\", \"source\": fname, \"text\": c})\n",
    "\n",
    "print(f\"Prepared {len(corpus)} chunks from {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1496ed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 29/29 [00:12<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built with 901 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) Embeddings and FAISS index\n",
    "\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "texts = [c[\"text\"] for c in corpus]\n",
    "embeddings = embedder.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "index = faiss.IndexFlatIP(EMBEDDING_DIM)\n",
    "index.add(embeddings)\n",
    "print(\"FAISS index built with\", index.ntotal, \"vectors\")\n",
    "\n",
    "faiss.write_index(index, INDEX_PATH)\n",
    "with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(corpus, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8373771f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.739046573638916, 'id': '1. Jharkhand Food and Feed Processing Policy 2024.pdf_chunk_44', 'source': '1. Jharkhand Food and Feed Processing Policy 2024.pdf', 'text': 'nt Subsidy (CPIS) in addition to what is already available under clause(s) 8.1, 8.2, 8.3 and 8.4 subject to the same maximum monetary limits. The Government of Jharkhand has identified the following products under the One District One Product Programme: Sl. No. District Name Product Name 1 Deoghar Dairy products 2 Dumka Dairy products 3 East Singhbhum Cashew Nut 4 Hazaribagh Jaggery 5 Jamtara Paddy based(Puffed rice, flattened rice, rice) 6 Khunti Tamarind 7 Koderma Ragi(Millet) 8 Latehar Mahua 9 Lohardaga Green Peas& other vegetable processing 10 Pakur Meat Processing 11 Ranchi Honey processing 12 Sahibganj Pickle 13 Saraikela Kharsawan Chironji 14 West Singhbhum Custard Apple 15 Bokaro Paddy/Rice based unit 16 Chatra Tomato 17 Dhanbad Paddy/Rice based unit 18 Garhwa Chilli 19 Giridih Mai'}, {'score': 0.7209710478782654, 'id': '7. Jharkhand Industrial and Investment Promotion Policy 2021.pdf_chunk_47', 'source': '7. Jharkhand Industrial and Investment Promotion Policy 2021.pdf', 'text': 'max Rs 5 Lakhs. Other Incentives Marketing Expansion/ modernization/ diversification Capital & Interest subsidy to MSME Stamp Duty &Registration fee Exemption For the details of the incentives, the Jharkhand Automobile & Auto Component Policy 2016 may be referred to. Jharkhand Gazette (Extraordinary), Tuesday, 13th July, 2021 11 3.1.3 AGRO-FOOD PROCESSING AND MEAT PROCESSING INDUSTRIES Jharkhand has huge untapped potential in Agriculture, Horticulture, Fishery, Animal Husbandry and Meat Processing Industry. The wide product base, a high volume of round the year production, strategic geographical location, abundant sunlight and high domestic demand project horticulture as the thrust area for development in Jharkhand. More than 5,000,000 MT vegetables are produced in Jharkhand, which is abou'}, {'score': 0.7017905712127686, 'id': '1. Jharkhand Food and Feed Processing Policy 2024.pdf_chunk_1', 'source': '1. Jharkhand Food and Feed Processing Policy 2024.pdf', 'text': 'lity), Goal 8 (Decent Work and Economic Growth) and Goal 10 (Reducing Inequalities). With its immense potential to enhance value addition and ‘farm-to-fork’ linkages, it is imperative that the sector is supported to help double farmers’ income and spread the benefits of economic development to the far- flung pockets of the state. Jharkhand Gazette (Extraordinary), Monday, 18th March, 2024 2 The State is blessed with bountiful production numbers in major cereals and horticultural crops and favourable for food industries to flourish and grow. Empirically, the Food and Feed Processing sector is a major employer of women and thus concerted efforts is required to promote this sector in the State which will further boost the Female Labour Force Participation Rate. As per Jharkhand Economic Surve'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6) Retriever\n",
    "\n",
    "def load_index_and_metadata(index_path: str = INDEX_PATH, metadata_path: str = METADATA_PATH):\n",
    "    idx = faiss.read_index(index_path)\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    return idx, meta\n",
    "\n",
    "\n",
    "index, metadata = load_index_and_metadata()\n",
    "\n",
    "def retrieve(query: str, k: int = 5):\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        meta = metadata[idx]\n",
    "        results.append({\"score\": float(score), \"id\": meta[\"id\"], \"source\": meta[\"source\"], \"text\": meta[\"text\"]})\n",
    "    return results\n",
    "\n",
    "print(retrieve(\"What are agriculture subsidies for Jharkhand?\", k=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0252afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) RAG with Gemini\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an assistant that answers questions about Jharkhand government policies. \"\n",
    "    \"Use only the provided source excerpts and cite source filenames. \"\n",
    "    \"If the answer is not present, say you don't know.\"\n",
    ")\n",
    "\n",
    "def generate_answer_gemini(question: str, contexts: List[Dict]) -> str:\n",
    "    context_block = \"\\n\\n\".join([f\"Source: {c['source']}\\n{c['text']}\" for c in contexts])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    {SYSTEM_PROMPT}\n",
    "\n",
    "    CONTEXT:\n",
    "    {context_block}\n",
    "\n",
    "    QUESTION: {question}\n",
    "\n",
    "    Provide a clear, concise answer with sources.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def answer_question(question: str, k: int = 5) -> Dict:\n",
    "    contexts = retrieve(question, k=k)\n",
    "    ans = generate_answer_gemini(question, contexts)\n",
    "    return {\"question\": question, \"answer\": ans, \"contexts\": contexts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e832668",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 3\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are agriculture subsidies for Jharkhand?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m, in \u001b[0;36manswer_question\u001b[1;34m(question, k)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manswer_question\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m, k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m     28\u001b[0m     contexts \u001b[38;5;241m=\u001b[39m retrieve(question, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m---> 29\u001b[0m     ans \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: ans, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m: contexts}\n",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m, in \u001b[0;36mgenerate_answer_gemini\u001b[1;34m(question, contexts)\u001b[0m\n\u001b[0;32m     12\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSYSTEM_PROMPT\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124mProvide a clear, concise answer with sources.\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m    332\u001b[0m             request,\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(next_sleep)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    210\u001b[0m         error_list,\n\u001b[0;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    212\u001b[0m         original_timeout,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 3\n}\n]"
     ]
    }
   ],
   "source": [
    "question = \"What are agriculture subsidies for Jharkhand?\"\n",
    "out = answer_question(question, k=5)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36484ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio UI ready. To launch, run: demo.launch()\n",
      "Index file: jharkhand_faiss.index\n",
      "Metadata file: jharkhand_metadata.json\n",
      "Notebook complete with Gemini integration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 667, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 349, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 2274, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1781, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19868\\2834302650.py\", line 10, in qa_fn\n",
      "    out = answer_question(question, k=5)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19868\\3961810915.py\", line 29, in answer_question\n",
      "    ans = generate_answer_gemini(question, contexts)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19868\\3961810915.py\", line 23, in generate_answer_gemini\n",
      "    response = model.generate_content(prompt)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\generativeai\\generative_models.py\", line 331, in generate_content\n",
      "    response = self._client.generate_content(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 667, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 349, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 2274, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1781, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19868\\2834302650.py\", line 10, in qa_fn\n",
      "    out = answer_question(question, k=5)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19868\\3961810915.py\", line 29, in answer_question\n",
      "    ans = generate_answer_gemini(question, contexts)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19868\\3961810915.py\", line 23, in generate_answer_gemini\n",
      "    response = model.generate_content(prompt)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\generativeai\\generative_models.py\", line 331, in generate_content\n",
      "    response = self._client.generate_content(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 50\n",
      "}\n",
      "]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 667, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 349, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 2274, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1781, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19868\\2834302650.py\", line 10, in qa_fn\n",
      "    out = answer_question(question, k=5)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19868\\3961810915.py\", line 29, in answer_question\n",
      "    ans = generate_answer_gemini(question, contexts)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19868\\3961810915.py\", line 23, in generate_answer_gemini\n",
      "    response = model.generate_content(prompt)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\generativeai\\generative_models.py\", line 331, in generate_content\n",
      "    response = self._client.generate_content(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\ASUS\\Desktop\\Major Project\\ML-KK\\.venv\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8) Gradio UI\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "except Exception:\n",
    "    gr = None\n",
    "\n",
    "if gr:\n",
    "    def qa_fn(question):\n",
    "        out = answer_question(question, k=5)\n",
    "        srcs = \"\\n\\n\".join([f\"[{i+1}] {c['source']} (score={c['score']:.3f})\\n{c['text'][:400]}...\" for i,c in enumerate(out['contexts'])])\n",
    "        return out['answer'], srcs\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Jharkhand Policies — RAG QnA (Gemini)\")\n",
    "        txt = gr.Textbox(lines=2, placeholder=\"Ask about Jharkhand policies...\", label=\"Question\")\n",
    "        out_ans = gr.Textbox(lines=8, label=\"Answer\")\n",
    "        out_ctx = gr.Textbox(lines=12, label=\"Retrieved contexts\")\n",
    "        btn = gr.Button(\"Ask\")\n",
    "        btn.click(fn=qa_fn, inputs=txt, outputs=[out_ans, out_ctx])\n",
    "\n",
    "    # To launch locally: demo.launch()\n",
    "    demo.launch()\n",
    "    print(\"Gradio UI ready. To launch, run: demo.launch()\")\n",
    "else:\n",
    "    print(\"Gradio not installed — skip UI cell.\")\n",
    "\n",
    "# %%\n",
    "# 9) Index and metadata\n",
    "\n",
    "print(\"Index file:\", INDEX_PATH)\n",
    "print(\"Metadata file:\", METADATA_PATH)\n",
    "print(\"Notebook complete with Gemini integration.\")\n",
    "# demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
